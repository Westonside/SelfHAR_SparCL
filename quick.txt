Using manual seed: 888
checkpoints/resnet18/paper/gradient_effi/mutate_irr/seq-cifar10/buffer_500//seed_888_75_derpp_0.80.txt
==============pre training=================
layer2.1.conv1.weight in config file cannot be found
layer3.1.conv1.weight in config file cannot be found
layer4.0.conv2.weight in config file cannot be found
layer1.1.conv1.weight in config file cannot be found
layer1.0.conv2.weight in config file cannot be found
layer4.1.conv2.weight in config file cannot be found
layer3.1.conv2.weight in config file cannot be found
layer2.0.conv2.weight in config file cannot be found
layer1.0.conv1.weight in config file cannot be found
layer4.0.conv1.weight in config file cannot be found
layer2.0.conv1.weight in config file cannot be found
layer3.0.conv2.weight in config file cannot be found
layer4.1.conv1.weight in config file cannot be found
layer2.1.conv2.weight in config file cannot be found
layer3.0.conv1.weight in config file cannot be found
layer1.1.conv2.weight in config file cannot be found
Hardened weight sparsity: name, num_nonzeros, total_num, sparsity
layer2.1.conv1.weight in config file cannot be found
layer3.1.conv1.weight in config file cannot be found
layer4.0.conv2.weight in config file cannot be found
layer1.1.conv1.weight in config file cannot be found
layer1.0.conv2.weight in config file cannot be found
layer4.1.conv2.weight in config file cannot be found
layer3.1.conv2.weight in config file cannot be found
layer2.0.conv2.weight in config file cannot be found
layer1.0.conv1.weight in config file cannot be found
layer4.0.conv1.weight in config file cannot be found
layer2.0.conv1.weight in config file cannot be found
layer3.0.conv2.weight in config file cannot be found
layer4.1.conv1.weight in config file cannot be found
layer2.1.conv2.weight in config file cannot be found
layer3.0.conv1.weight in config file cannot be found
layer1.1.conv2.weight in config file cannot be found
!!!!! upper_bound 0.74-0.75-0.75
!!!!! lower_bound 0.75-0.76-0.75
fc1.weight: 49152, 49152, 0.0
fc1.weight: sparsity too low, skip
fc1.bias: 512, 512, 0.0
fc1.bias: sparsity too low, skip
fc2.weight: 3072, 3072, 0.0
fc2.weight: sparsity too low, skip
fc2.bias: 6, 6, 0.0
fc2.bias: sparsity too low, skip
The sparsity of all parameters: name, num_nonzeros, total_num, shape, sparsity
fc1.weight: 49152, 49152, (512, 96), [0.0]
fc1.bias: 512, 512, (512,), [0.0]
fc2.weight: 3072, 3072, (6, 512), [0.0]
fc2.bias: 6, 6, (6,), [0.0]
---------------------------------------------------------------------------
total number of zeros: 0, non-zeros: 0, zero sparsity is: 0.0000
0
===========================================================================


****************************************************************************************************
dataset Namespace(arch='simple', depth=18, workers=4, multi_gpu=False, s=0.0001, batch_size=32, test_batch_size=256, epochs=250, optmzr='sgd', lr=0.03, lr_decay=60, momentum=0.9, weight_decay=0.0001, no_cuda=False, seed=888, lr_scheduler='cosine', warmup=False, warmup_lr=0.0001, warmup_epochs=0, mixup=False, alpha=0.3, smooth=False, smooth_eps=0.0, log_interval=10, rho=0.0001, pretrain_epochs=0, pruning_epochs=0, remark='irr_0.75_mut', save_model='checkpoints/resnet18/paper/gradient_effi/mutate_irr/seq-cifar10/buffer_500/', sparsity_type='random-pattern', config_file='config_vgg16', use_cl_mask=True, buffer_size=500, buffer_weight=0.1, buffer_weight_beta=0.5, dataset='hhar_features', validation=True, test_epoch_interval=1, evaluate_mode=False, eval_checkpoint=None, gradient_efficient=False, gradient_efficient_mix=True, gradient_remove=0.1, gradient_sparse=0.8, sample_frequency=30, replay_method='derpp', patternNum=8, rand_seed=False, log_filename='checkpoints/resnet18/paper/gradient_effi/mutate_irr/seq-cifar10/buffer_500//seed_888_75_derpp_0.80.txt', resume=None, save_mask_model=False, mask_sparsity=None, output_dir='checkpoints/resnet18/paper/gradient_effi/mutate_irr/seq-cifar10/buffer_500/', output_name='irr_0.75_mut_RM_3000_20', remove_data_epoch=20, data_augmentation=False, remove_n=3000, keep_lowest_n=0, sorting_file=None, input_dir='.', sp_retrain=True, sp_config_file='./profiles/resnet18_cifar/irr/resnet18_0.75.yaml', sp_no_harden=False, sp_admm_sparsity_type='irregular', sp_load_frozen_weights=None, retrain_mask_pattern='weight', sp_update_init_method='zero', sp_mask_update_freq=5, sp_lmd=0.5, retrain_mask_sparsity=-1.0, retrain_mask_seed=None, sp_prune_before_retrain=True, output_compressed_format=False, sp_grad_update=False, sp_grad_decay=0.98, sp_grad_restore_threshold=-1, sp_global_magnitude=False, sp_pre_defined_mask_dir=None, upper_bound='0.74-0.75-0.75', lower_bound='0.75-0.76-0.75', mask_update_decay_epoch='5-45', cuda=True)
**********Inspecting hhar_features**********
**********Initializing buffer**********
testing 
testing 
(12690, 96)
len(train_dataset.targets) 12690
Training on 12690 examples
Epoch: [0][0/397]	LR: 0.03000	Loss 0.9388	Acc@1 0.000	Time 0.664 (0.664)	Data 0.000 (0.000)	
Epoch: [0][10/397]	LR: 0.03000	Loss 0.3204	Acc@1 41.193	Time 0.012 (0.069)	Data 0.002 (0.002)	
Epoch: [0][20/397]	LR: 0.03000	Loss 0.1867	Acc@1 50.595	Time 0.010 (0.041)	Data 0.002 (0.002)	
Epoch: [0][30/397]	LR: 0.03000	Loss 0.2126	Acc@1 57.964	Time 0.009 (0.031)	Data 0.002 (0.002)	
Epoch: [0][40/397]	LR: 0.03000	Loss 0.1490	Acc@1 63.948	Time 0.009 (0.025)	Data 0.001 (0.001)	
Epoch: [0][50/397]	LR: 0.03000	Loss 0.2353	Acc@1 67.647	Time 0.009 (0.022)	Data 0.001 (0.001)	
Epoch: [0][60/397]	LR: 0.03000	Loss 0.0785	Acc@1 70.441	Time 0.017 (0.021)	Data 0.001 (0.001)	
Epoch: [0][70/397]	LR: 0.03000	Loss 0.1808	Acc@1 72.007	Time 0.009 (0.021)	Data 0.001 (0.001)	
Epoch: [0][80/397]	LR: 0.03000	Loss 0.0743	Acc@1 73.495	Time 0.018 (0.020)	Data 0.001 (0.001)	
Epoch: [0][90/397]	LR: 0.03000	Loss 0.0887	Acc@1 74.622	Time 0.015 (0.020)	Data 0.001 (0.001)	
Epoch: [0][100/397]	LR: 0.03000	Loss 0.1217	Acc@1 76.145	Time 0.009 (0.019)	Data 0.000 (0.001)	
Epoch: [0][110/397]	LR: 0.03000	Loss 0.1492	Acc@1 77.196	Time 0.009 (0.018)	Data 0.000 (0.001)	
Epoch: [0][120/397]	LR: 0.03000	Loss 0.0587	Acc@1 78.151	Time 0.015 (0.018)	Data 0.000 (0.001)	
Epoch: [0][130/397]	LR: 0.03000	Loss 0.1793	Acc@1 78.841	Time 0.015 (0.018)	Data 0.000 (0.001)	
Epoch: [0][140/397]	LR: 0.03000	Loss 0.1156	Acc@1 79.322	Time 0.016 (0.018)	Data 0.001 (0.001)	
Epoch: [0][150/397]	LR: 0.03000	Loss 0.0742	Acc@1 80.070	Time 0.016 (0.018)	Data 0.000 (0.001)	
Epoch: [0][160/397]	LR: 0.03000	Loss 0.1934	Acc@1 80.745	Time 0.021 (0.017)	Data 0.000 (0.001)	
Epoch: [0][170/397]	LR: 0.03000	Loss 0.0845	Acc@1 81.287	Time 0.016 (0.018)	Data 0.000 (0.001)	
Epoch: [0][180/397]	LR: 0.03000	Loss 0.1444	Acc@1 81.682	Time 0.025 (0.018)	Data 0.001 (0.001)	
Epoch: [0][190/397]	LR: 0.03000	Loss 0.1340	Acc@1 82.003	Time 0.015 (0.018)	Data 0.000 (0.001)	
Epoch: [0][200/397]	LR: 0.03000	Loss 0.0793	Acc@1 82.323	Time 0.024 (0.018)	Data 0.000 (0.001)	
Epoch: [0][210/397]	LR: 0.03000	Loss 0.0412	Acc@1 82.731	Time 0.020 (0.019)	Data 0.000 (0.001)	
Epoch: [0][220/397]	LR: 0.03000	Loss 0.0802	Acc@1 83.117	Time 0.024 (0.019)	Data 0.000 (0.001)	
Epoch: [0][230/397]	LR: 0.03000	Loss 0.0738	Acc@1 83.333	Time 0.023 (0.019)	Data 0.000 (0.001)	
Epoch: [0][240/397]	LR: 0.03000	Loss 0.1351	Acc@1 83.454	Time 0.034 (0.019)	Data 0.000 (0.001)	
Epoch: [0][250/397]	LR: 0.03000	Loss 0.1099	Acc@1 83.678	Time 0.009 (0.019)	Data 0.000 (0.001)	
Epoch: [0][260/397]	LR: 0.03000	Loss 0.0377	Acc@1 84.064	Time 0.027 (0.019)	Data 0.000 (0.001)	
Epoch: [0][270/397]	LR: 0.03000	Loss 0.0911	Acc@1 84.271	Time 0.023 (0.019)	Data 0.000 (0.001)	
Epoch: [0][280/397]	LR: 0.03000	Loss 0.2210	Acc@1 84.486	Time 0.020 (0.019)	Data 0.000 (0.001)	
Epoch: [0][290/397]	LR: 0.03000	Loss 0.0630	Acc@1 84.719	Time 0.008 (0.018)	Data 0.000 (0.001)	
Epoch: [0][300/397]	LR: 0.03000	Loss 0.2600	Acc@1 84.967	Time 0.008 (0.018)	Data 0.000 (0.001)	
Epoch: [0][310/397]	LR: 0.03000	Loss 0.1151	Acc@1 85.169	Time 0.008 (0.018)	Data 0.000 (0.001)	
Epoch: [0][320/397]	LR: 0.03000	Loss 0.0603	Acc@1 85.300	Time 0.008 (0.018)	Data 0.000 (0.001)	
Epoch: [0][330/397]	LR: 0.03000	Loss 0.0964	Acc@1 85.404	Time 0.012 (0.017)	Data 0.000 (0.001)	
Epoch: [0][340/397]	LR: 0.03000	Loss 0.1442	Acc@1 85.502	Time 0.008 (0.017)	Data 0.000 (0.001)	
Epoch: [0][350/397]	LR: 0.03000	Loss 0.0557	Acc@1 85.541	Time 0.008 (0.017)	Data 0.000 (0.001)	
Epoch: [0][360/397]	LR: 0.03000	Loss 0.0637	Acc@1 85.725	Time 0.011 (0.017)	Data 0.000 (0.001)	
Epoch: [0][370/397]	LR: 0.03000	Loss 0.0178	Acc@1 85.773	Time 0.008 (0.016)	Data 0.000 (0.001)	
Epoch: [0][380/397]	LR: 0.03000	Loss 0.1070	Acc@1 85.925	Time 0.008 (0.016)	Data 0.000 (0.001)	
Epoch: [0][390/397]	LR: 0.03000	Loss 0.0526	Acc@1 86.093	Time 0.009 (0.016)	Data 0.000 (0.001)	
The sparsity of all parameters: name, num_nonzeros, total_num, shape, sparsity
fc1.weight: 49152, 49152, (512, 96), [0.0]
fc1.bias: 512, 512, (512,), [0.0]
fc2.weight: 3072, 3072, (6, 512), [0.0]
fc2.bias: 6, 6, (6,), [0.0]
---------------------------------------------------------------------------
0.0
===========================================================================


Training on 12690 examples
Epoch: [1][0/397]	LR: 0.03000	Loss 0.0732	Acc@1 93.750	Time 0.010 (0.010)	Data 0.000 (0.000)	
Epoch: [1][10/397]	LR: 0.03000	Loss 0.0701	Acc@1 91.477	Time 0.008 (0.009)	Data 0.000 (0.000)	
Epoch: [1][20/397]	LR: 0.03000	Loss 0.0800	Acc@1 90.625	Time 0.008 (0.010)	Data 0.000 (0.000)	
Epoch: [1][30/397]	LR: 0.03000	Loss 0.0326	Acc@1 91.532	Time 0.008 (0.010)	Data 0.000 (0.000)	
Epoch: [1][40/397]	LR: 0.03000	Loss 0.0493	Acc@1 91.540	Time 0.008 (0.010)	Data 0.000 (0.000)	
Epoch: [1][50/397]	LR: 0.03000	Loss 0.1239	Acc@1 91.483	Time 0.008 (0.010)	Data 0.000 (0.000)	
Epoch: [1][60/397]	LR: 0.03000	Loss 0.0953	Acc@1 91.957	Time 0.008 (0.009)	Data 0.000 (0.000)	
Epoch: [1][70/397]	LR: 0.03000	Loss 0.0619	Acc@1 92.165	Time 0.009 (0.009)	Data 0.000 (0.000)	
Epoch: [1][80/397]	LR: 0.03000	Loss 0.0754	Acc@1 92.361	Time 0.008 (0.009)	Data 0.000 (0.000)	
Epoch: [1][90/397]	LR: 0.03000	Loss 0.0633	Acc@1 92.479	Time 0.008 (0.009)	Data 0.000 (0.000)	
Epoch: [1][100/397]	LR: 0.03000	Loss 0.0093	Acc@1 92.884	Time 0.008 (0.009)	Data 0.000 (0.000)	
Epoch: [1][110/397]	LR: 0.03000	Loss 0.1751	Acc@1 93.018	Time 0.008 (0.009)	Data 0.000 (0.000)	
Epoch: [1][120/397]	LR: 0.03000	Loss 0.1505	Acc@1 93.001	Time 0.008 (0.009)	Data 0.000 (0.000)	
Epoch: [1][130/397]	LR: 0.03000	Loss 0.0370	Acc@1 92.891	Time 0.008 (0.009)	Data 0.000 (0.000)	
Epoch: [1][140/397]	LR: 0.03000	Loss 0.0384	Acc@1 93.041	Time 0.008 (0.009)	Data 0.000 (0.000)	
Epoch: [1][150/397]	LR: 0.03000	Loss 0.0481	Acc@1 93.026	Time 0.008 (0.009)	Data 0.000 (0.000)	
Epoch: [1][160/397]	LR: 0.03000	Loss 0.0344	Acc@1 93.109	Time 0.008 (0.009)	Data 0.000 (0.000)	
Epoch: [1][170/397]	LR: 0.03000	Loss 0.0057	Acc@1 93.037	Time 0.008 (0.009)	Data 0.000 (0.000)	
Epoch: [1][180/397]	LR: 0.03000	Loss 0.0726	Acc@1 92.990	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][190/397]	LR: 0.03000	Loss 0.0231	Acc@1 93.046	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][200/397]	LR: 0.03000	Loss 0.0235	Acc@1 93.097	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][210/397]	LR: 0.03000	Loss 0.0381	Acc@1 93.054	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][220/397]	LR: 0.03000	Loss 0.0282	Acc@1 93.015	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][230/397]	LR: 0.03000	Loss 0.0653	Acc@1 93.114	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][240/397]	LR: 0.03000	Loss 0.0427	Acc@1 93.050	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][250/397]	LR: 0.03000	Loss 0.1153	Acc@1 93.028	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][260/397]	LR: 0.03000	Loss 0.0249	Acc@1 93.115	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][270/397]	LR: 0.03000	Loss 0.0307	Acc@1 93.127	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][280/397]	LR: 0.03000	Loss 0.0202	Acc@1 93.138	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][290/397]	LR: 0.03000	Loss 0.0278	Acc@1 93.063	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][300/397]	LR: 0.03000	Loss 0.1583	Acc@1 93.065	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][310/397]	LR: 0.03000	Loss 0.0721	Acc@1 93.137	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][320/397]	LR: 0.03000	Loss 0.0448	Acc@1 93.215	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][330/397]	LR: 0.03000	Loss 0.0847	Acc@1 93.202	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][340/397]	LR: 0.03000	Loss 0.0085	Acc@1 93.237	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][350/397]	LR: 0.03000	Loss 0.1353	Acc@1 93.323	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][360/397]	LR: 0.03000	Loss 0.0282	Acc@1 93.369	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][370/397]	LR: 0.03000	Loss 0.0363	Acc@1 93.329	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][380/397]	LR: 0.03000	Loss 0.0200	Acc@1 93.299	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][390/397]	LR: 0.03000	Loss 0.0201	Acc@1 93.302	Time 0.007 (0.008)	Data 0.000 (0.000)	
The sparsity of all parameters: name, num_nonzeros, total_num, shape, sparsity
fc1.weight: 49152, 49152, (512, 96), [0.0]
fc1.bias: 512, 512, (512,), [0.0]
fc2.weight: 3072, 3072, (6, 512), [0.0]
fc2.bias: 6, 6, (6,), [0.0]
---------------------------------------------------------------------------
0.0
===========================================================================


Training on 12690 examples
Epoch: [2][0/397]	LR: 0.03000	Loss 0.1104	Acc@1 87.500	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][10/397]	LR: 0.03000	Loss 0.0411	Acc@1 94.318	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [2][20/397]	LR: 0.03000	Loss 0.0415	Acc@1 93.452	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [2][30/397]	LR: 0.03000	Loss 0.1058	Acc@1 92.944	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [2][40/397]	LR: 0.03000	Loss 0.0913	Acc@1 93.216	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][50/397]	LR: 0.03000	Loss 0.0412	Acc@1 93.505	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][60/397]	LR: 0.03000	Loss 0.0908	Acc@1 93.596	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][70/397]	LR: 0.03000	Loss 0.0667	Acc@1 93.530	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][80/397]	LR: 0.03000	Loss 0.0368	Acc@1 93.711	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][90/397]	LR: 0.03000	Loss 0.0684	Acc@1 94.162	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][100/397]	LR: 0.03000	Loss 0.0693	Acc@1 94.152	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [2][110/397]	LR: 0.03000	Loss 0.0357	Acc@1 94.200	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][120/397]	LR: 0.03000	Loss 0.0110	Acc@1 94.370	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [2][130/397]	LR: 0.03000	Loss 0.1923	Acc@1 94.370	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][140/397]	LR: 0.03000	Loss 0.0146	Acc@1 94.415	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [2][150/397]	LR: 0.03000	Loss 0.0310	Acc@1 94.495	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [2][160/397]	LR: 0.03000	Loss 0.0181	Acc@1 94.585	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [2][170/397]	LR: 0.03000	Loss 0.1247	Acc@1 94.572	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [2][180/397]	LR: 0.03000	Loss 0.0144	Acc@1 94.561	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [2][190/397]	LR: 0.03000	Loss 0.0273	Acc@1 94.421	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [2][200/397]	LR: 0.03000	Loss 0.0320	Acc@1 94.512	Time 0.011 (0.008)	Data 0.000 (0.000)	
Epoch: [2][210/397]	LR: 0.03000	Loss 0.0209	Acc@1 94.594	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [2][220/397]	LR: 0.03000	Loss 0.0335	Acc@1 94.514	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [2][230/397]	LR: 0.03000	Loss 0.1041	Acc@1 94.508	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [2][240/397]	LR: 0.03000	Loss 0.0086	Acc@1 94.554	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [2][250/397]	LR: 0.03000	Loss 0.0089	Acc@1 94.584	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [2][260/397]	LR: 0.03000	Loss 0.0238	Acc@1 94.624	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [2][270/397]	LR: 0.03000	Loss 0.0256	Acc@1 94.661	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [2][280/397]	LR: 0.03000	Loss 0.0285	Acc@1 94.718	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [2][290/397]	LR: 0.03000	Loss 0.0501	Acc@1 94.770	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [2][300/397]	LR: 0.03000	Loss 0.0817	Acc@1 94.809	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [2][310/397]	LR: 0.03000	Loss 0.1629	Acc@1 94.815	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [2][320/397]	LR: 0.03000	Loss 0.0914	Acc@1 94.840	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [2][330/397]	LR: 0.03000	Loss 0.0279	Acc@1 94.911	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [2][340/397]	LR: 0.03000	Loss 0.0866	Acc@1 94.923	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [2][350/397]	LR: 0.03000	Loss 0.0389	Acc@1 94.970	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [2][360/397]	LR: 0.03000	Loss 0.0304	Acc@1 95.014	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [2][370/397]	LR: 0.03000	Loss 0.0015	Acc@1 95.013	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [2][380/397]	LR: 0.03000	Loss 0.0350	Acc@1 95.054	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [2][390/397]	LR: 0.03000	Loss 0.0264	Acc@1 95.069	Time 0.008 (0.008)	Data 0.000 (0.000)	
The sparsity of all parameters: name, num_nonzeros, total_num, shape, sparsity
fc1.weight: 49152, 49152, (512, 96), [0.0]
fc1.bias: 512, 512, (512,), [0.0]
fc2.weight: 3072, 3072, (6, 512), [0.0]
fc2.bias: 6, 6, (6,), [0.0]
---------------------------------------------------------------------------
0.0
===========================================================================


Training on 12690 examples
Epoch: [3][0/397]	LR: 0.03000	Loss 0.0241	Acc@1 93.750	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][10/397]	LR: 0.03000	Loss 0.0502	Acc@1 92.045	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][20/397]	LR: 0.03000	Loss 0.0291	Acc@1 94.345	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [3][30/397]	LR: 0.03000	Loss 0.0348	Acc@1 95.060	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][40/397]	LR: 0.03000	Loss 0.0374	Acc@1 95.351	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][50/397]	LR: 0.03000	Loss 0.0925	Acc@1 95.527	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][60/397]	LR: 0.03000	Loss 0.0079	Acc@1 95.645	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][70/397]	LR: 0.03000	Loss 0.0605	Acc@1 95.819	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][80/397]	LR: 0.03000	Loss 0.0716	Acc@1 95.640	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [3][90/397]	LR: 0.03000	Loss 0.0742	Acc@1 95.501	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][100/397]	LR: 0.03000	Loss 0.1048	Acc@1 95.483	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][110/397]	LR: 0.03000	Loss 0.0182	Acc@1 95.495	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][120/397]	LR: 0.03000	Loss 0.0483	Acc@1 95.506	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][130/397]	LR: 0.03000	Loss 0.0158	Acc@1 95.587	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][140/397]	LR: 0.03000	Loss 0.0107	Acc@1 95.612	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][150/397]	LR: 0.03000	Loss 0.0134	Acc@1 95.716	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][160/397]	LR: 0.03000	Loss 0.0086	Acc@1 95.691	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][170/397]	LR: 0.03000	Loss 0.0194	Acc@1 95.632	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][180/397]	LR: 0.03000	Loss 0.0339	Acc@1 95.632	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][190/397]	LR: 0.03000	Loss 0.0204	Acc@1 95.632	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [3][200/397]	LR: 0.03000	Loss 0.0169	Acc@1 95.616	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][210/397]	LR: 0.03000	Loss 0.0649	Acc@1 95.616	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][220/397]	LR: 0.03000	Loss 0.0494	Acc@1 95.659	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][230/397]	LR: 0.03000	Loss 0.2295	Acc@1 95.630	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [3][240/397]	LR: 0.03000	Loss 0.0228	Acc@1 95.708	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][250/397]	LR: 0.03000	Loss 0.0393	Acc@1 95.692	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][260/397]	LR: 0.03000	Loss 0.0082	Acc@1 95.750	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [3][270/397]	LR: 0.03000	Loss 0.0734	Acc@1 95.687	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][280/397]	LR: 0.03000	Loss 0.0086	Acc@1 95.741	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [3][290/397]	LR: 0.03000	Loss 0.0174	Acc@1 95.801	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [3][300/397]	LR: 0.03000	Loss 0.1012	Acc@1 95.764	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][310/397]	LR: 0.03000	Loss 0.0281	Acc@1 95.770	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [3][320/397]	LR: 0.03000	Loss 0.0117	Acc@1 95.843	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [3][330/397]	LR: 0.03000	Loss 0.0259	Acc@1 95.884	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][340/397]	LR: 0.03000	Loss 0.0847	Acc@1 95.885	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][350/397]	LR: 0.03000	Loss 0.0256	Acc@1 95.958	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [3][360/397]	LR: 0.03000	Loss 0.0259	Acc@1 95.957	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][370/397]	LR: 0.03000	Loss 0.0150	Acc@1 95.957	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][380/397]	LR: 0.03000	Loss 0.0068	Acc@1 95.956	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][390/397]	LR: 0.03000	Loss 0.0229	Acc@1 95.964	Time 0.007 (0.007)	Data 0.000 (0.000)	
The sparsity of all parameters: name, num_nonzeros, total_num, shape, sparsity
fc1.weight: 49152, 49152, (512, 96), [0.0]
fc1.bias: 512, 512, (512,), [0.0]
fc2.weight: 3072, 3072, (6, 512), [0.0]
fc2.bias: 6, 6, (6,), [0.0]
---------------------------------------------------------------------------
0.0
===========================================================================


Training on 12690 examples
Epoch: [4][0/397]	LR: 0.03000	Loss 0.0264	Acc@1 90.625	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][10/397]	LR: 0.03000	Loss 0.0408	Acc@1 95.170	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [4][20/397]	LR: 0.03000	Loss 0.0070	Acc@1 95.536	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [4][30/397]	LR: 0.03000	Loss 0.1735	Acc@1 95.665	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [4][40/397]	LR: 0.03000	Loss 0.0072	Acc@1 95.732	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][50/397]	LR: 0.03000	Loss 0.0056	Acc@1 96.140	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][60/397]	LR: 0.03000	Loss 0.0847	Acc@1 96.004	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [4][70/397]	LR: 0.03000	Loss 0.0054	Acc@1 96.127	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][80/397]	LR: 0.03000	Loss 0.0201	Acc@1 96.258	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][90/397]	LR: 0.03000	Loss 0.0218	Acc@1 96.360	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][100/397]	LR: 0.03000	Loss 0.0672	Acc@1 96.504	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [4][110/397]	LR: 0.03000	Loss 0.2449	Acc@1 96.622	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][120/397]	LR: 0.03000	Loss 0.0184	Acc@1 96.643	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][130/397]	LR: 0.03000	Loss 0.0189	Acc@1 96.589	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][140/397]	LR: 0.03000	Loss 0.0084	Acc@1 96.698	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][150/397]	LR: 0.03000	Loss 0.0361	Acc@1 96.751	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][160/397]	LR: 0.03000	Loss 0.0136	Acc@1 96.759	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [4][170/397]	LR: 0.03000	Loss 0.0092	Acc@1 96.838	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][180/397]	LR: 0.03000	Loss 0.0311	Acc@1 96.789	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][190/397]	LR: 0.03000	Loss 0.0150	Acc@1 96.760	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][200/397]	LR: 0.03000	Loss 0.0254	Acc@1 96.751	Time 0.009 (0.007)	Data 0.000 (0.000)	
Epoch: [4][210/397]	LR: 0.03000	Loss 0.0091	Acc@1 96.697	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][220/397]	LR: 0.03000	Loss 0.0327	Acc@1 96.691	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][230/397]	LR: 0.03000	Loss 0.1155	Acc@1 96.699	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [4][240/397]	LR: 0.03000	Loss 0.0445	Acc@1 96.655	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][250/397]	LR: 0.03000	Loss 0.0185	Acc@1 96.701	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][260/397]	LR: 0.03000	Loss 0.0529	Acc@1 96.624	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [4][270/397]	LR: 0.03000	Loss 0.0077	Acc@1 96.656	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [4][280/397]	LR: 0.03000	Loss 0.0067	Acc@1 96.653	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][290/397]	LR: 0.03000	Loss 0.0290	Acc@1 96.639	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [4][300/397]	LR: 0.03000	Loss 0.0112	Acc@1 96.657	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][310/397]	LR: 0.03000	Loss 0.0385	Acc@1 96.694	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][320/397]	LR: 0.03000	Loss 0.0132	Acc@1 96.671	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [4][330/397]	LR: 0.03000	Loss 0.0860	Acc@1 96.639	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [4][340/397]	LR: 0.03000	Loss 0.0041	Acc@1 96.655	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [4][350/397]	LR: 0.03000	Loss 0.0228	Acc@1 96.661	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][360/397]	LR: 0.03000	Loss 0.0077	Acc@1 96.633	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][370/397]	LR: 0.03000	Loss 0.0813	Acc@1 96.639	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][380/397]	LR: 0.03000	Loss 0.0413	Acc@1 96.629	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [4][390/397]	LR: 0.03000	Loss 0.0465	Acc@1 96.627	Time 0.008 (0.007)	Data 0.000 (0.000)	
The sparsity of all parameters: name, num_nonzeros, total_num, shape, sparsity
fc1.weight: 49152, 49152, (512, 96), [0.0]
fc1.bias: 512, 512, (512,), [0.0]
fc2.weight: 3072, 3072, (6, 512), [0.0]
fc2.bias: 6, 6, (6,), [0.0]
---------------------------------------------------------------------------
0.0
===========================================================================


epoch 5
len(diag_stats.items()) 12691
unlearned_per_presentation 12690
first_learned 12690
unlearned_per_presentation_all 1
first_learned_all 1
Number of unforgettable examples: 12086
epoch before ordered_examples len 12690
epoch before len(train_dataset.targets) 12690
elements_to_remove 750
removed train_indx 11940
shape (11940, 96)
len(train_dataset.targets) 11940
Training on 11940 examples
Epoch: [5][0/374]	LR: 0.03000	Loss 0.0062	Acc@1 100.000	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [5][10/374]	LR: 0.03000	Loss 0.0283	Acc@1 96.307	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [5][20/374]	LR: 0.03000	Loss 0.1115	Acc@1 96.726	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [5][30/374]	LR: 0.03000	Loss 0.0177	Acc@1 96.573	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [5][40/374]	LR: 0.03000	Loss 0.0396	Acc@1 96.341	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [5][50/374]	LR: 0.03000	Loss 0.0110	Acc@1 96.446	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [5][60/374]	LR: 0.03000	Loss 0.0074	Acc@1 96.619	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [5][70/374]	LR: 0.03000	Loss 0.0092	Acc@1 96.523	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [5][80/374]	LR: 0.03000	Loss 0.0267	Acc@1 96.682	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [5][90/374]	LR: 0.03000	Loss 0.0714	Acc@1 96.806	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [5][100/374]	LR: 0.03000	Loss 0.0340	Acc@1 96.751	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [5][110/374]	LR: 0.03000	Loss 0.0883	Acc@1 96.734	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [5][120/374]	LR: 0.03000	Loss 0.0573	Acc@1 96.823	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [5][130/374]	LR: 0.03000	Loss 0.0094	Acc@1 96.803	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [5][140/374]	LR: 0.03000	Loss 0.0154	Acc@1 96.831	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [5][150/374]	LR: 0.03000	Loss 0.0123	Acc@1 96.751	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [5][160/374]	LR: 0.03000	Loss 0.0025	Acc@1 96.817	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [5][170/374]	LR: 0.03000	Loss 0.0186	Acc@1 96.820	Time 0.055 (0.008)	Data 0.000 (0.000)	
Epoch: [5][180/374]	LR: 0.03000	Loss 0.0010	Acc@1 96.754	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [5][190/374]	LR: 0.03000	Loss 0.0054	Acc@1 96.810	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [5][200/374]	LR: 0.03000	Loss 0.0131	Acc@1 96.891	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [5][210/374]	LR: 0.03000	Loss 0.0107	Acc@1 96.919	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [5][220/374]	LR: 0.03000	Loss 0.0057	Acc@1 97.031	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [5][230/374]	LR: 0.03000	Loss 0.0079	Acc@1 97.037	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [5][240/374]	LR: 0.03000	Loss 0.0669	Acc@1 97.018	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [5][250/374]	LR: 0.03000	Loss 0.0720	Acc@1 96.912	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [5][260/374]	LR: 0.03000	Loss 0.0666	Acc@1 96.887	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [5][270/374]	LR: 0.03000	Loss 0.0070	Acc@1 96.863	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [5][280/374]	LR: 0.03000	Loss 0.0233	Acc@1 96.808	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [5][290/374]	LR: 0.03000	Loss 0.0152	Acc@1 96.832	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [5][300/374]	LR: 0.03000	Loss 0.0163	Acc@1 96.833	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [5][310/374]	LR: 0.03000	Loss 0.2240	Acc@1 96.845	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [5][320/374]	LR: 0.03000	Loss 0.0144	Acc@1 96.894	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [5][330/374]	LR: 0.03000	Loss 0.0142	Acc@1 96.913	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [5][340/374]	LR: 0.03000	Loss 0.0160	Acc@1 96.902	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [5][350/374]	LR: 0.03000	Loss 0.0090	Acc@1 96.920	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [5][360/374]	LR: 0.03000	Loss 0.0438	Acc@1 96.970	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [5][370/374]	LR: 0.03000	Loss 0.0192	Acc@1 96.993	Time 0.007 (0.008)	Data 0.000 (0.000)	
The sparsity of all parameters: name, num_nonzeros, total_num, shape, sparsity
fc1.weight: 49152, 49152, (512, 96), [0.0]
fc1.bias: 512, 512, (512,), [0.0]
fc2.weight: 3072, 3072, (6, 512), [0.0]
fc2.bias: 6, 6, (6,), [0.0]
---------------------------------------------------------------------------
0.0
===========================================================================


Training on 11940 examples
Epoch: [6][0/374]	LR: 0.03000	Loss 0.0250	Acc@1 96.875	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][10/374]	LR: 0.03000	Loss 0.0035	Acc@1 96.023	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][20/374]	LR: 0.03000	Loss 0.0199	Acc@1 96.131	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][30/374]	LR: 0.03000	Loss 0.0141	Acc@1 96.169	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][40/374]	LR: 0.03000	Loss 0.0510	Acc@1 96.418	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [6][50/374]	LR: 0.03000	Loss 0.0064	Acc@1 96.691	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][60/374]	LR: 0.03000	Loss 0.0059	Acc@1 96.670	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][70/374]	LR: 0.03000	Loss 0.1047	Acc@1 96.523	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [6][80/374]	LR: 0.03000	Loss 0.0108	Acc@1 96.489	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [6][90/374]	LR: 0.03000	Loss 0.0383	Acc@1 96.669	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [6][100/374]	LR: 0.03000	Loss 0.0106	Acc@1 96.627	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][110/374]	LR: 0.03000	Loss 0.0029	Acc@1 96.622	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][120/374]	LR: 0.03000	Loss 0.0065	Acc@1 96.720	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][130/374]	LR: 0.03000	Loss 0.0023	Acc@1 96.684	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][140/374]	LR: 0.03000	Loss 0.0115	Acc@1 96.786	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][150/374]	LR: 0.03000	Loss 0.1224	Acc@1 96.916	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [6][160/374]	LR: 0.03000	Loss 0.0054	Acc@1 97.030	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][170/374]	LR: 0.03000	Loss 0.0131	Acc@1 97.039	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][180/374]	LR: 0.03000	Loss 0.0185	Acc@1 97.099	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [6][190/374]	LR: 0.03000	Loss 0.0491	Acc@1 97.039	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [6][200/374]	LR: 0.03000	Loss 0.0387	Acc@1 97.093	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][210/374]	LR: 0.03000	Loss 0.0095	Acc@1 97.142	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [6][220/374]	LR: 0.03000	Loss 0.0092	Acc@1 97.073	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][230/374]	LR: 0.03000	Loss 0.0103	Acc@1 97.064	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][240/374]	LR: 0.03000	Loss 0.0138	Acc@1 97.095	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [6][250/374]	LR: 0.03000	Loss 0.0102	Acc@1 97.099	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][260/374]	LR: 0.03000	Loss 0.0053	Acc@1 97.114	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][270/374]	LR: 0.03000	Loss 0.0269	Acc@1 97.152	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [6][280/374]	LR: 0.03000	Loss 0.0053	Acc@1 97.175	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][290/374]	LR: 0.03000	Loss 0.0017	Acc@1 97.197	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][300/374]	LR: 0.03000	Loss 0.0223	Acc@1 97.228	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][310/374]	LR: 0.03000	Loss 0.0191	Acc@1 97.207	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][320/374]	LR: 0.03000	Loss 0.0768	Acc@1 97.216	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][330/374]	LR: 0.03000	Loss 0.0126	Acc@1 97.196	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][340/374]	LR: 0.03000	Loss 0.0188	Acc@1 97.205	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [6][350/374]	LR: 0.03000	Loss 0.0065	Acc@1 97.178	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [6][360/374]	LR: 0.03000	Loss 0.0097	Acc@1 97.213	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [6][370/374]	LR: 0.03000	Loss 0.0251	Acc@1 97.262	Time 0.007 (0.007)	Data 0.000 (0.000)	
The sparsity of all parameters: name, num_nonzeros, total_num, shape, sparsity
fc1.weight: 49152, 49152, (512, 96), [0.0]
fc1.bias: 512, 512, (512,), [0.0]
fc2.weight: 3072, 3072, (6, 512), [0.0]
fc2.bias: 6, 6, (6,), [0.0]
---------------------------------------------------------------------------
0.0
===========================================================================


Training on 11940 examples
Epoch: [7][0/374]	LR: 0.03000	Loss 0.0033	Acc@1 100.000	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][10/374]	LR: 0.03000	Loss 0.0216	Acc@1 98.295	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][20/374]	LR: 0.03000	Loss 0.0333	Acc@1 97.917	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][30/374]	LR: 0.03000	Loss 0.0174	Acc@1 97.581	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][40/374]	LR: 0.03000	Loss 0.0787	Acc@1 97.637	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][50/374]	LR: 0.03000	Loss 0.0451	Acc@1 97.672	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][60/374]	LR: 0.03000	Loss 0.0407	Acc@1 97.746	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][70/374]	LR: 0.03000	Loss 0.0165	Acc@1 97.667	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][80/374]	LR: 0.03000	Loss 0.0255	Acc@1 97.724	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][90/374]	LR: 0.03000	Loss 0.0295	Acc@1 97.665	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][100/374]	LR: 0.03000	Loss 0.0639	Acc@1 97.525	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][110/374]	LR: 0.03000	Loss 0.0125	Acc@1 97.354	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][120/374]	LR: 0.03000	Loss 0.0081	Acc@1 97.417	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][130/374]	LR: 0.03000	Loss 0.0086	Acc@1 97.495	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][140/374]	LR: 0.03000	Loss 0.0171	Acc@1 97.496	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [7][150/374]	LR: 0.03000	Loss 0.0023	Acc@1 97.413	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][160/374]	LR: 0.03000	Loss 0.0222	Acc@1 97.457	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][170/374]	LR: 0.03000	Loss 0.0029	Acc@1 97.387	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [7][180/374]	LR: 0.03000	Loss 0.0285	Acc@1 97.358	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][190/374]	LR: 0.03000	Loss 0.0064	Acc@1 97.366	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][200/374]	LR: 0.03000	Loss 0.0094	Acc@1 97.373	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][210/374]	LR: 0.03000	Loss 0.0021	Acc@1 97.408	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][220/374]	LR: 0.03000	Loss 0.0074	Acc@1 97.483	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][230/374]	LR: 0.03000	Loss 0.0133	Acc@1 97.484	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][240/374]	LR: 0.03000	Loss 0.0209	Acc@1 97.497	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][250/374]	LR: 0.03000	Loss 0.0051	Acc@1 97.510	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [7][260/374]	LR: 0.03000	Loss 0.0130	Acc@1 97.522	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][270/374]	LR: 0.03000	Loss 0.0114	Acc@1 97.475	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [7][280/374]	LR: 0.03000	Loss 0.0060	Acc@1 97.542	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][290/374]	LR: 0.03000	Loss 0.0280	Acc@1 97.562	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][300/374]	LR: 0.03000	Loss 0.0015	Acc@1 97.560	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][310/374]	LR: 0.03000	Loss 0.0068	Acc@1 97.568	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][320/374]	LR: 0.03000	Loss 0.0559	Acc@1 97.595	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][330/374]	LR: 0.03000	Loss 0.0018	Acc@1 97.602	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][340/374]	LR: 0.03000	Loss 0.0063	Acc@1 97.626	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][350/374]	LR: 0.03000	Loss 0.0153	Acc@1 97.596	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [7][360/374]	LR: 0.03000	Loss 0.0096	Acc@1 97.637	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [7][370/374]	LR: 0.03000	Loss 0.0016	Acc@1 97.650	Time 0.007 (0.007)	Data 0.000 (0.000)	
The sparsity of all parameters: name, num_nonzeros, total_num, shape, sparsity
fc1.weight: 49152, 49152, (512, 96), [0.0]
fc1.bias: 512, 512, (512,), [0.0]
fc2.weight: 3072, 3072, (6, 512), [0.0]
fc2.bias: 6, 6, (6,), [0.0]
---------------------------------------------------------------------------
0.0
===========================================================================


Training on 11940 examples
Epoch: [8][0/374]	LR: 0.03000	Loss 0.0091	Acc@1 100.000	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][10/374]	LR: 0.03000	Loss 0.0219	Acc@1 97.159	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][20/374]	LR: 0.03000	Loss 0.0171	Acc@1 97.917	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][30/374]	LR: 0.03000	Loss 0.0397	Acc@1 97.581	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][40/374]	LR: 0.03000	Loss 0.0067	Acc@1 97.790	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [8][50/374]	LR: 0.03000	Loss 0.0109	Acc@1 97.610	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][60/374]	LR: 0.03000	Loss 0.0062	Acc@1 97.592	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][70/374]	LR: 0.03000	Loss 0.0233	Acc@1 97.667	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][80/374]	LR: 0.03000	Loss 0.0079	Acc@1 97.608	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [8][90/374]	LR: 0.03000	Loss 0.0168	Acc@1 97.493	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][100/374]	LR: 0.03000	Loss 0.0028	Acc@1 97.463	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][110/374]	LR: 0.03000	Loss 0.0050	Acc@1 97.494	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][120/374]	LR: 0.03000	Loss 0.0184	Acc@1 97.469	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [8][130/374]	LR: 0.03000	Loss 0.0006	Acc@1 97.591	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][140/374]	LR: 0.03000	Loss 0.0101	Acc@1 97.651	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][150/374]	LR: 0.03000	Loss 0.0055	Acc@1 97.765	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [8][160/374]	LR: 0.03000	Loss 0.0251	Acc@1 97.748	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][170/374]	LR: 0.03000	Loss 0.0189	Acc@1 97.716	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [8][180/374]	LR: 0.03000	Loss 0.0031	Acc@1 97.756	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [8][190/374]	LR: 0.03000	Loss 0.0185	Acc@1 97.759	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][200/374]	LR: 0.03000	Loss 0.0170	Acc@1 97.839	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][210/374]	LR: 0.03000	Loss 0.0038	Acc@1 97.941	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][220/374]	LR: 0.03000	Loss 0.0056	Acc@1 97.950	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][230/374]	LR: 0.03000	Loss 0.0099	Acc@1 97.930	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][240/374]	LR: 0.03000	Loss 0.0107	Acc@1 97.964	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][250/374]	LR: 0.03000	Loss 0.0177	Acc@1 97.958	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][260/374]	LR: 0.03000	Loss 0.0007	Acc@1 97.941	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][270/374]	LR: 0.03000	Loss 0.0094	Acc@1 97.959	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][280/374]	LR: 0.03000	Loss 0.0137	Acc@1 97.965	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][290/374]	LR: 0.03000	Loss 0.0120	Acc@1 97.927	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][300/374]	LR: 0.03000	Loss 0.0152	Acc@1 97.934	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][310/374]	LR: 0.03000	Loss 0.0363	Acc@1 97.900	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][320/374]	LR: 0.03000	Loss 0.0136	Acc@1 97.887	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][330/374]	LR: 0.03000	Loss 0.0396	Acc@1 97.904	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][340/374]	LR: 0.03000	Loss 0.0053	Acc@1 97.920	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][350/374]	LR: 0.03000	Loss 0.0091	Acc@1 97.952	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][360/374]	LR: 0.03000	Loss 0.0034	Acc@1 97.974	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [8][370/374]	LR: 0.03000	Loss 0.0079	Acc@1 97.987	Time 0.007 (0.007)	Data 0.000 (0.000)	
The sparsity of all parameters: name, num_nonzeros, total_num, shape, sparsity
fc1.weight: 49152, 49152, (512, 96), [0.0]
fc1.bias: 512, 512, (512,), [0.0]
fc2.weight: 3072, 3072, (6, 512), [0.0]
fc2.bias: 6, 6, (6,), [0.0]
---------------------------------------------------------------------------
0.0
===========================================================================


Training on 11940 examples
Epoch: [9][0/374]	LR: 0.03000	Loss 0.0504	Acc@1 96.875	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][10/374]	LR: 0.03000	Loss 0.0113	Acc@1 98.864	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][20/374]	LR: 0.03000	Loss 0.0102	Acc@1 98.810	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][30/374]	LR: 0.03000	Loss 0.0046	Acc@1 98.589	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][40/374]	LR: 0.03000	Loss 0.0128	Acc@1 98.247	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][50/374]	LR: 0.03000	Loss 0.0034	Acc@1 98.162	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [9][60/374]	LR: 0.03000	Loss 0.0287	Acc@1 98.105	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][70/374]	LR: 0.03000	Loss 0.0051	Acc@1 98.151	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [9][80/374]	LR: 0.03000	Loss 0.0350	Acc@1 98.110	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][90/374]	LR: 0.03000	Loss 0.0290	Acc@1 98.111	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][100/374]	LR: 0.03000	Loss 0.0204	Acc@1 97.958	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][110/374]	LR: 0.03000	Loss 0.0095	Acc@1 97.917	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][120/374]	LR: 0.03000	Loss 0.0031	Acc@1 97.908	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][130/374]	LR: 0.03000	Loss 0.0183	Acc@1 97.972	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [9][140/374]	LR: 0.03000	Loss 0.0943	Acc@1 98.005	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][150/374]	LR: 0.03000	Loss 0.0031	Acc@1 97.993	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [9][160/374]	LR: 0.03000	Loss 0.0018	Acc@1 97.943	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [9][170/374]	LR: 0.03000	Loss 0.0203	Acc@1 97.971	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][180/374]	LR: 0.03000	Loss 0.0367	Acc@1 98.015	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][190/374]	LR: 0.03000	Loss 0.0289	Acc@1 98.020	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [9][200/374]	LR: 0.03000	Loss 0.0015	Acc@1 98.010	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [9][210/374]	LR: 0.03000	Loss 0.0132	Acc@1 98.060	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][220/374]	LR: 0.03000	Loss 0.0036	Acc@1 98.077	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][230/374]	LR: 0.03000	Loss 0.0198	Acc@1 98.065	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][240/374]	LR: 0.03000	Loss 0.0176	Acc@1 98.068	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][250/374]	LR: 0.03000	Loss 0.0047	Acc@1 98.045	Time 0.009 (0.007)	Data 0.000 (0.000)	
Epoch: [9][260/374]	LR: 0.03000	Loss 0.0015	Acc@1 98.060	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][270/374]	LR: 0.03000	Loss 0.0025	Acc@1 98.097	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][280/374]	LR: 0.03000	Loss 0.0065	Acc@1 98.109	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [9][290/374]	LR: 0.03000	Loss 0.0067	Acc@1 98.099	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][300/374]	LR: 0.03000	Loss 0.0120	Acc@1 98.100	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][310/374]	LR: 0.03000	Loss 0.0081	Acc@1 98.101	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][320/374]	LR: 0.03000	Loss 0.0021	Acc@1 98.131	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [9][330/374]	LR: 0.03000	Loss 0.0075	Acc@1 98.140	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][340/374]	LR: 0.03000	Loss 0.0450	Acc@1 98.112	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][350/374]	LR: 0.03000	Loss 0.0012	Acc@1 98.139	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][360/374]	LR: 0.03000	Loss 0.0037	Acc@1 98.173	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [9][370/374]	LR: 0.03000	Loss 0.0075	Acc@1 98.197	Time 0.007 (0.007)	Data 0.000 (0.000)	
The sparsity of all parameters: name, num_nonzeros, total_num, shape, sparsity
fc1.weight: 49152, 49152, (512, 96), [0.0]
fc1.bias: 512, 512, (512,), [0.0]
fc2.weight: 3072, 3072, (6, 512), [0.0]
fc2.bias: 6, 6, (6,), [0.0]
---------------------------------------------------------------------------
0.0
===========================================================================


epoch 10
len(diag_stats.items()) 11941
unlearned_per_presentation 11940
first_learned 11940
unlearned_per_presentation_all 1
first_learned_all 1
Number of unforgettable examples: 11564
epoch before ordered_examples len 11940
epoch before len(train_dataset.targets) 11940
elements_to_remove 750
removed train_indx 11190
shape (11190, 96)
len(train_dataset.targets) 11190
Training on 11190 examples
Epoch: [10][0/350]	LR: 0.03000	Loss 0.0255	Acc@1 96.875	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][10/350]	LR: 0.03000	Loss 0.0023	Acc@1 98.580	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][20/350]	LR: 0.03000	Loss 0.0171	Acc@1 98.661	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][30/350]	LR: 0.03000	Loss 0.0005	Acc@1 98.589	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][40/350]	LR: 0.03000	Loss 0.0060	Acc@1 98.857	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][50/350]	LR: 0.03000	Loss 0.0011	Acc@1 98.713	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [10][60/350]	LR: 0.03000	Loss 0.0019	Acc@1 98.668	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][70/350]	LR: 0.03000	Loss 0.0103	Acc@1 98.415	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][80/350]	LR: 0.03000	Loss 0.0150	Acc@1 98.534	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][90/350]	LR: 0.03000	Loss 0.0008	Acc@1 98.420	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][100/350]	LR: 0.03000	Loss 0.0103	Acc@1 98.422	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][110/350]	LR: 0.03000	Loss 0.0077	Acc@1 98.423	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][120/350]	LR: 0.03000	Loss 0.0180	Acc@1 98.450	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][130/350]	LR: 0.03000	Loss 0.0097	Acc@1 98.473	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [10][140/350]	LR: 0.03000	Loss 0.0324	Acc@1 98.382	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][150/350]	LR: 0.03000	Loss 0.0085	Acc@1 98.406	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][160/350]	LR: 0.03000	Loss 0.1489	Acc@1 98.408	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [10][170/350]	LR: 0.03000	Loss 0.0102	Acc@1 98.410	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][180/350]	LR: 0.03000	Loss 0.0063	Acc@1 98.394	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][190/350]	LR: 0.03000	Loss 0.0284	Acc@1 98.380	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][200/350]	LR: 0.03000	Loss 0.0079	Acc@1 98.368	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][210/350]	LR: 0.03000	Loss 0.0018	Acc@1 98.356	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][220/350]	LR: 0.03000	Loss 0.0030	Acc@1 98.388	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [10][230/350]	LR: 0.03000	Loss 0.0052	Acc@1 98.363	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [10][240/350]	LR: 0.03000	Loss 0.0026	Acc@1 98.301	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][250/350]	LR: 0.03000	Loss 0.0151	Acc@1 98.232	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][260/350]	LR: 0.03000	Loss 0.0361	Acc@1 98.216	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][270/350]	LR: 0.03000	Loss 0.0800	Acc@1 98.167	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][280/350]	LR: 0.03000	Loss 0.0042	Acc@1 98.154	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][290/350]	LR: 0.03000	Loss 0.0088	Acc@1 98.174	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][300/350]	LR: 0.03000	Loss 0.0057	Acc@1 98.204	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][310/350]	LR: 0.03000	Loss 0.0175	Acc@1 98.201	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][320/350]	LR: 0.03000	Loss 0.0023	Acc@1 98.218	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [10][330/350]	LR: 0.03000	Loss 0.0103	Acc@1 98.225	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [10][340/350]	LR: 0.03000	Loss 0.0169	Acc@1 98.222	Time 0.008 (0.007)	Data 0.000 (0.000)	
The sparsity of all parameters: name, num_nonzeros, total_num, shape, sparsity
fc1.weight: 49152, 49152, (512, 96), [0.0]
fc1.bias: 512, 512, (512,), [0.0]
fc2.weight: 3072, 3072, (6, 512), [0.0]
fc2.bias: 6, 6, (6,), [0.0]
---------------------------------------------------------------------------
0.0
===========================================================================


Training on 11190 examples
Epoch: [11][0/350]	LR: 0.03000	Loss 0.0037	Acc@1 100.000	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][10/350]	LR: 0.03000	Loss 0.0382	Acc@1 98.580	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][20/350]	LR: 0.03000	Loss 0.0060	Acc@1 98.512	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [11][30/350]	LR: 0.03000	Loss 0.1254	Acc@1 98.387	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][40/350]	LR: 0.03000	Loss 0.1218	Acc@1 98.018	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][50/350]	LR: 0.03000	Loss 0.0062	Acc@1 98.039	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][60/350]	LR: 0.03000	Loss 0.0079	Acc@1 98.258	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][70/350]	LR: 0.03000	Loss 0.0030	Acc@1 98.151	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][80/350]	LR: 0.03000	Loss 0.0359	Acc@1 98.225	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][90/350]	LR: 0.03000	Loss 0.0148	Acc@1 98.077	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][100/350]	LR: 0.03000	Loss 0.0045	Acc@1 98.113	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][110/350]	LR: 0.03000	Loss 0.0229	Acc@1 98.114	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][120/350]	LR: 0.03000	Loss 0.0020	Acc@1 98.218	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][130/350]	LR: 0.03000	Loss 0.0046	Acc@1 98.235	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][140/350]	LR: 0.03000	Loss 0.0026	Acc@1 98.249	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][150/350]	LR: 0.03000	Loss 0.0060	Acc@1 98.262	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][160/350]	LR: 0.03000	Loss 0.0129	Acc@1 98.253	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][170/350]	LR: 0.03000	Loss 0.0003	Acc@1 98.300	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][180/350]	LR: 0.03000	Loss 0.0007	Acc@1 98.377	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [11][190/350]	LR: 0.03000	Loss 0.0070	Acc@1 98.413	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [11][200/350]	LR: 0.03000	Loss 0.0295	Acc@1 98.399	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][210/350]	LR: 0.03000	Loss 0.0014	Acc@1 98.430	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][220/350]	LR: 0.03000	Loss 0.0043	Acc@1 98.388	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][230/350]	LR: 0.03000	Loss 0.0100	Acc@1 98.417	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [11][240/350]	LR: 0.03000	Loss 0.0036	Acc@1 98.470	Time 0.007 (0.007)	Data 0.000 (0.000)	
[2 3 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 0task 1[2 3 4 5][0 1 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 1[0 1 4 5]task 2[0 1 2 3]task 0task 0[2 3 4 5]task 1[0 1 4 5]task 2[0 1 2 3]task 0task 1[2 3 4 5]task 2[0 1 4 5][0 1 2 3]task 0task 0[2 3 4 5]task 0task 1[2 3 4 5]task 2[0 1 4 5][0 1 2 3]task 0[2 3 4 5]task 1[0 1 4 5]task 0[2 3 4 5]task 1[0 1 4 5]task 0[2 3 4 5]task 1[0 1 4 5]task 0[2 3 4 5]task 1task 0[2 3 4 5]task 0[2 3 4 5]task 1[0 1 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 1[0 1 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 1[0 1 4 5]task 0[2 3 4 5]task 1[0 1 4 5]task 0[2 3 4 5]task 1[0 1 4 5]task 0[2 3 4 5]task 1[0 1 4 5]task 0[2 3 4 5]task 1[0 1 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 0task 0[2 3 4 5]task 0[2 3 4 5]task 0task 0[2 3 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 0[2 3 4 5]task 0task 0task 0[2 3 4 5]task 0[2 3 4 5]task 0[2 3 4 5][2 3 4 5][2 3 4 5][2 3 4 5][2 3 4 5][2 3 4 5][2 3 4 5][2 3 4 5][2 3 4 5][2 3 4 5][2 3 4 5][2 3 4 5][2 3 4 5][0 1 4 5][0 1 2 3][2 3 4 5][0 1 4 5][0 1 2 3]