Using manual seed: 888
checkpoints/resnet18/paper/gradient_effi/mutate_irr/seq-cifar10/buffer_500//seed_888_75_derpp_0.80.txt
==============pre training=================
layer4.1.conv2.weight in config file cannot be found
layer2.0.conv1.weight in config file cannot be found
layer3.1.conv1.weight in config file cannot be found
layer3.0.conv2.weight in config file cannot be found
layer1.0.conv1.weight in config file cannot be found
layer1.0.conv2.weight in config file cannot be found
layer2.0.conv2.weight in config file cannot be found
layer4.0.conv1.weight in config file cannot be found
layer1.1.conv2.weight in config file cannot be found
layer4.0.conv2.weight in config file cannot be found
layer3.1.conv2.weight in config file cannot be found
layer2.1.conv1.weight in config file cannot be found
layer4.1.conv1.weight in config file cannot be found
layer3.0.conv1.weight in config file cannot be found
layer2.1.conv2.weight in config file cannot be found
layer1.1.conv1.weight in config file cannot be found
Hardened weight sparsity: name, num_nonzeros, total_num, sparsity
layer4.1.conv2.weight in config file cannot be found
layer2.0.conv1.weight in config file cannot be found
layer3.1.conv1.weight in config file cannot be found
layer3.0.conv2.weight in config file cannot be found
layer1.0.conv1.weight in config file cannot be found
layer1.0.conv2.weight in config file cannot be found
layer2.0.conv2.weight in config file cannot be found
layer4.0.conv1.weight in config file cannot be found
layer1.1.conv2.weight in config file cannot be found
layer4.0.conv2.weight in config file cannot be found
layer3.1.conv2.weight in config file cannot be found
layer2.1.conv1.weight in config file cannot be found
layer4.1.conv1.weight in config file cannot be found
layer3.0.conv1.weight in config file cannot be found
layer2.1.conv2.weight in config file cannot be found
layer1.1.conv1.weight in config file cannot be found
!!!!! upper_bound 0.74-0.75-0.75
!!!!! lower_bound 0.75-0.76-0.75
fc1.weight: 49152, 49152, 0.0
fc1.weight: sparsity too low, skip
fc1.bias: 512, 512, 0.0
fc1.bias: sparsity too low, skip
fc2.weight: 3072, 3072, 0.0
fc2.weight: sparsity too low, skip
fc2.bias: 6, 6, 0.0
fc2.bias: sparsity too low, skip
The sparsity of all parameters: name, num_nonzeros, total_num, shape, sparsity
fc1.weight: 49152, 49152, (512, 96), [0.0]
fc1.bias: 512, 512, (512,), [0.0]
fc2.weight: 3072, 3072, (6, 512), [0.0]
fc2.bias: 6, 6, (6,), [0.0]
---------------------------------------------------------------------------
total number of zeros: 0, non-zeros: 0, zero sparsity is: 0.0000
0
===========================================================================


****************************************************************************************************
dataset Namespace(arch='simple', depth=18, workers=4, multi_gpu=False, s=0.0001, batch_size=32, test_batch_size=256, epochs=250, optmzr='sgd', lr=0.03, lr_decay=60, momentum=0.9, weight_decay=0.0001, no_cuda=False, seed=888, lr_scheduler='cosine', warmup=False, warmup_lr=0.0001, warmup_epochs=0, mixup=False, alpha=0.3, smooth=False, smooth_eps=0.0, log_interval=10, rho=0.0001, pretrain_epochs=0, pruning_epochs=0, remark='irr_0.75_mut', save_model='checkpoints/resnet18/paper/gradient_effi/mutate_irr/seq-cifar10/buffer_500/', sparsity_type='random-pattern', config_file='config_vgg16', use_cl_mask=True, buffer_size=500, buffer_weight=0.1, buffer_weight_beta=0.5, dataset='hhar_features', validation=True, test_epoch_interval=1, evaluate_mode=False, eval_checkpoint=None, gradient_efficient=False, gradient_efficient_mix=True, gradient_remove=0.1, gradient_sparse=0.8, sample_frequency=30, replay_method='derpp', patternNum=8, rand_seed=False, log_filename='checkpoints/resnet18/paper/gradient_effi/mutate_irr/seq-cifar10/buffer_500//seed_888_75_derpp_0.80.txt', resume=None, save_mask_model=False, mask_sparsity=None, output_dir='checkpoints/resnet18/paper/gradient_effi/mutate_irr/seq-cifar10/buffer_500/', output_name='irr_0.75_mut_RM_3000_20', remove_data_epoch=20, data_augmentation=False, remove_n=3000, keep_lowest_n=0, sorting_file=None, input_dir='.', sp_retrain=True, sp_config_file='./profiles/resnet18_cifar/irr/resnet18_0.75.yaml', sp_no_harden=False, sp_admm_sparsity_type='irregular', sp_load_frozen_weights=None, retrain_mask_pattern='weight', sp_update_init_method='zero', sp_mask_update_freq=5, sp_lmd=0.5, retrain_mask_sparsity=-1.0, retrain_mask_seed=None, sp_prune_before_retrain=True, output_compressed_format=False, sp_grad_update=False, sp_grad_decay=0.98, sp_grad_restore_threshold=-1, sp_global_magnitude=False, sp_pre_defined_mask_dir=None, upper_bound='0.74-0.75-0.75', lower_bound='0.75-0.76-0.75', mask_update_decay_epoch='5-45', cuda=True)
**********Inspecting hhar_features**********
**********Initializing buffer**********
testing 
testing 
(12685, 96)
len(train_dataset.targets) 12685
Training on 12685 examples
Epoch: [0][0/397]	LR: 0.03000	Loss 0.8050	Acc@1 0.000	Time 0.600 (0.600)	Data 0.000 (0.000)	
Epoch: [0][10/397]	LR: 0.03000	Loss 0.3591	Acc@1 41.761	Time 0.009 (0.063)	Data 0.002 (0.002)	
Epoch: [0][20/397]	LR: 0.03000	Loss 0.1875	Acc@1 52.827	Time 0.009 (0.037)	Data 0.002 (0.002)	
Epoch: [0][30/397]	LR: 0.03000	Loss 0.1717	Acc@1 57.661	Time 0.009 (0.028)	Data 0.001 (0.002)	
Epoch: [0][40/397]	LR: 0.03000	Loss 0.2200	Acc@1 61.585	Time 0.008 (0.023)	Data 0.001 (0.001)	
Epoch: [0][50/397]	LR: 0.03000	Loss 0.2720	Acc@1 65.441	Time 0.008 (0.020)	Data 0.001 (0.001)	
Epoch: [0][60/397]	LR: 0.03000	Loss 0.2681	Acc@1 67.879	Time 0.008 (0.018)	Data 0.001 (0.001)	
Epoch: [0][70/397]	LR: 0.03000	Loss 0.1021	Acc@1 69.762	Time 0.008 (0.017)	Data 0.001 (0.001)	
Epoch: [0][80/397]	LR: 0.03000	Loss 0.1460	Acc@1 71.412	Time 0.008 (0.016)	Data 0.001 (0.001)	
Epoch: [0][90/397]	LR: 0.03000	Loss 0.1272	Acc@1 72.699	Time 0.008 (0.015)	Data 0.000 (0.001)	
Epoch: [0][100/397]	LR: 0.03000	Loss 0.3204	Acc@1 73.484	Time 0.008 (0.014)	Data 0.000 (0.001)	
Epoch: [0][110/397]	LR: 0.03000	Loss 0.0708	Acc@1 74.493	Time 0.008 (0.013)	Data 0.001 (0.001)	
Epoch: [0][120/397]	LR: 0.03000	Loss 0.1613	Acc@1 75.258	Time 0.007 (0.013)	Data 0.000 (0.001)	
Epoch: [0][130/397]	LR: 0.03000	Loss 0.1280	Acc@1 76.240	Time 0.008 (0.013)	Data 0.001 (0.001)	
Epoch: [0][140/397]	LR: 0.03000	Loss 0.0563	Acc@1 77.283	Time 0.008 (0.012)	Data 0.000 (0.001)	
Epoch: [0][150/397]	LR: 0.03000	Loss 0.1585	Acc@1 78.084	Time 0.008 (0.012)	Data 0.000 (0.001)	
Epoch: [0][160/397]	LR: 0.03000	Loss 0.0629	Acc@1 78.901	Time 0.007 (0.012)	Data 0.000 (0.001)	
Epoch: [0][170/397]	LR: 0.03000	Loss 0.1091	Acc@1 79.386	Time 0.008 (0.011)	Data 0.000 (0.001)	
Epoch: [0][180/397]	LR: 0.03000	Loss 0.0471	Acc@1 79.921	Time 0.007 (0.011)	Data 0.000 (0.001)	
Epoch: [0][190/397]	LR: 0.03000	Loss 0.0483	Acc@1 80.546	Time 0.007 (0.011)	Data 0.000 (0.001)	
Epoch: [0][200/397]	LR: 0.03000	Loss 0.1511	Acc@1 81.001	Time 0.008 (0.011)	Data 0.000 (0.001)	
Epoch: [0][210/397]	LR: 0.03000	Loss 0.0585	Acc@1 81.176	Time 0.007 (0.011)	Data 0.000 (0.001)	
Epoch: [0][220/397]	LR: 0.03000	Loss 0.0309	Acc@1 81.505	Time 0.007 (0.011)	Data 0.000 (0.001)	
Epoch: [0][230/397]	LR: 0.03000	Loss 0.0600	Acc@1 81.805	Time 0.009 (0.010)	Data 0.000 (0.001)	
Epoch: [0][240/397]	LR: 0.03000	Loss 0.0313	Acc@1 82.171	Time 0.007 (0.010)	Data 0.000 (0.001)	
Epoch: [0][250/397]	LR: 0.03000	Loss 0.1193	Acc@1 82.520	Time 0.007 (0.010)	Data 0.000 (0.001)	
Epoch: [0][260/397]	LR: 0.03000	Loss 0.0219	Acc@1 82.795	Time 0.007 (0.010)	Data 0.000 (0.001)	
Epoch: [0][270/397]	LR: 0.03000	Loss 0.1011	Acc@1 83.095	Time 0.008 (0.010)	Data 0.000 (0.001)	
Epoch: [0][280/397]	LR: 0.03000	Loss 0.0810	Acc@1 83.352	Time 0.008 (0.010)	Data 0.000 (0.001)	
Epoch: [0][290/397]	LR: 0.03000	Loss 0.0502	Acc@1 83.527	Time 0.007 (0.010)	Data 0.000 (0.001)	
Epoch: [0][300/397]	LR: 0.03000	Loss 0.0867	Acc@1 83.648	Time 0.008 (0.010)	Data 0.000 (0.001)	
Epoch: [0][310/397]	LR: 0.03000	Loss 0.0463	Acc@1 83.873	Time 0.007 (0.010)	Data 0.000 (0.001)	
Epoch: [0][320/397]	LR: 0.03000	Loss 0.1495	Acc@1 84.025	Time 0.007 (0.010)	Data 0.000 (0.001)	
Epoch: [0][330/397]	LR: 0.03000	Loss 0.0349	Acc@1 84.281	Time 0.007 (0.010)	Data 0.000 (0.000)	
Epoch: [0][340/397]	LR: 0.03000	Loss 0.1473	Acc@1 84.485	Time 0.008 (0.009)	Data 0.000 (0.000)	
Epoch: [0][350/397]	LR: 0.03000	Loss 0.1068	Acc@1 84.696	Time 0.007 (0.009)	Data 0.000 (0.000)	
Epoch: [0][360/397]	LR: 0.03000	Loss 0.1122	Acc@1 84.972	Time 0.007 (0.009)	Data 0.000 (0.000)	
Epoch: [0][370/397]	LR: 0.03000	Loss 0.0628	Acc@1 85.108	Time 0.008 (0.009)	Data 0.000 (0.000)	
Epoch: [0][380/397]	LR: 0.03000	Loss 0.0430	Acc@1 85.228	Time 0.008 (0.009)	Data 0.000 (0.000)	
Epoch: [0][390/397]	LR: 0.03000	Loss 0.0285	Acc@1 85.390	Time 0.008 (0.009)	Data 0.000 (0.000)	
The sparsity of all parameters: name, num_nonzeros, total_num, shape, sparsity
fc1.weight: 49152, 49152, (512, 96), [0.0]
fc1.bias: 512, 512, (512,), [0.0]
fc2.weight: 3072, 3072, (6, 512), [0.0]
fc2.bias: 6, 6, (6,), [0.0]
---------------------------------------------------------------------------
0.0
===========================================================================


Training on 12685 examples
Task: 0, Epoch:0, Average Acc:[92.341], , Task Inc Acc:[96.513], Learning Acc:[92.341], Forgetting:[0.000], LR:0.03
	Acc@T0: 92.341	
	Til-Acc@T0: 96.513	

Training on 12685 examples
Epoch: [1][0/397]	LR: 0.03000	Loss 0.1599	Acc@1 90.625	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][10/397]	LR: 0.03000	Loss 0.0251	Acc@1 93.182	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][20/397]	LR: 0.03000	Loss 0.1324	Acc@1 91.667	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][30/397]	LR: 0.03000	Loss 0.0829	Acc@1 92.339	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][40/397]	LR: 0.03000	Loss 0.0365	Acc@1 92.378	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][50/397]	LR: 0.03000	Loss 0.0581	Acc@1 92.279	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][60/397]	LR: 0.03000	Loss 0.0505	Acc@1 92.674	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][70/397]	LR: 0.03000	Loss 0.1321	Acc@1 92.870	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][80/397]	LR: 0.03000	Loss 0.0813	Acc@1 92.940	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][90/397]	LR: 0.03000	Loss 0.0410	Acc@1 92.857	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][100/397]	LR: 0.03000	Loss 0.0934	Acc@1 92.884	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][110/397]	LR: 0.03000	Loss 0.1037	Acc@1 92.736	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][120/397]	LR: 0.03000	Loss 0.0392	Acc@1 92.588	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][130/397]	LR: 0.03000	Loss 0.0647	Acc@1 92.677	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][140/397]	LR: 0.03000	Loss 0.0961	Acc@1 92.730	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][150/397]	LR: 0.03000	Loss 0.0513	Acc@1 92.632	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][160/397]	LR: 0.03000	Loss 0.0262	Acc@1 92.624	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][170/397]	LR: 0.03000	Loss 0.0144	Acc@1 92.599	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][180/397]	LR: 0.03000	Loss 0.0720	Acc@1 92.680	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][190/397]	LR: 0.03000	Loss 0.0201	Acc@1 92.670	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][200/397]	LR: 0.03000	Loss 0.0193	Acc@1 92.802	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][210/397]	LR: 0.03000	Loss 0.0477	Acc@1 92.817	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][220/397]	LR: 0.03000	Loss 0.0462	Acc@1 92.817	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][230/397]	LR: 0.03000	Loss 0.0738	Acc@1 92.762	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][240/397]	LR: 0.03000	Loss 0.0241	Acc@1 92.777	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][250/397]	LR: 0.03000	Loss 0.0163	Acc@1 92.779	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][260/397]	LR: 0.03000	Loss 0.0373	Acc@1 92.792	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][270/397]	LR: 0.03000	Loss 0.0352	Acc@1 92.804	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][280/397]	LR: 0.03000	Loss 0.0505	Acc@1 92.849	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][290/397]	LR: 0.03000	Loss 0.0715	Acc@1 92.912	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][300/397]	LR: 0.03000	Loss 0.0662	Acc@1 92.971	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][310/397]	LR: 0.03000	Loss 0.0065	Acc@1 93.016	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][320/397]	LR: 0.03000	Loss 0.0727	Acc@1 93.078	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][330/397]	LR: 0.03000	Loss 0.0406	Acc@1 93.108	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [1][340/397]	LR: 0.03000	Loss 0.0386	Acc@1 93.127	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [1][350/397]	LR: 0.03000	Loss 0.0288	Acc@1 93.145	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [1][360/397]	LR: 0.03000	Loss 0.0574	Acc@1 93.161	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [1][370/397]	LR: 0.03000	Loss 0.0335	Acc@1 93.169	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [1][380/397]	LR: 0.03000	Loss 0.1150	Acc@1 93.200	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [1][390/397]	LR: 0.03000	Loss 0.1227	Acc@1 93.183	Time 0.009 (0.007)	Data 0.000 (0.000)	
The sparsity of all parameters: name, num_nonzeros, total_num, shape, sparsity
fc1.weight: 49152, 49152, (512, 96), [0.0]
fc1.bias: 512, 512, (512,), [0.0]
fc2.weight: 3072, 3072, (6, 512), [0.0]
fc2.bias: 6, 6, (6,), [0.0]
---------------------------------------------------------------------------
0.0
===========================================================================


Training on 12685 examples
Task: 0, Epoch:1, Average Acc:[94.147], , Task Inc Acc:[98.568], Learning Acc:[94.147], Forgetting:[0.000], LR:0.03
	Acc@T0: 94.147	
	Til-Acc@T0: 98.568	

Training on 12685 examples
Epoch: [2][0/397]	LR: 0.03000	Loss 0.0278	Acc@1 96.875	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [2][10/397]	LR: 0.03000	Loss 0.0569	Acc@1 95.170	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [2][20/397]	LR: 0.03000	Loss 0.0330	Acc@1 94.940	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [2][30/397]	LR: 0.03000	Loss 0.0882	Acc@1 94.556	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [2][40/397]	LR: 0.03000	Loss 0.0232	Acc@1 94.741	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [2][50/397]	LR: 0.03000	Loss 0.0548	Acc@1 95.098	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [2][60/397]	LR: 0.03000	Loss 0.0148	Acc@1 94.980	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [2][70/397]	LR: 0.03000	Loss 0.0168	Acc@1 94.850	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [2][80/397]	LR: 0.03000	Loss 0.1544	Acc@1 94.907	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [2][90/397]	LR: 0.03000	Loss 0.0770	Acc@1 94.883	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [2][100/397]	LR: 0.03000	Loss 0.2150	Acc@1 94.678	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][110/397]	LR: 0.03000	Loss 0.0061	Acc@1 94.792	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][120/397]	LR: 0.03000	Loss 0.0418	Acc@1 94.835	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][130/397]	LR: 0.03000	Loss 0.0613	Acc@1 94.871	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][140/397]	LR: 0.03000	Loss 0.0511	Acc@1 94.969	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][150/397]	LR: 0.03000	Loss 0.0110	Acc@1 94.805	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][160/397]	LR: 0.03000	Loss 0.0122	Acc@1 94.818	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][170/397]	LR: 0.03000	Loss 0.0164	Acc@1 94.920	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][180/397]	LR: 0.03000	Loss 0.0722	Acc@1 94.993	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][190/397]	LR: 0.03000	Loss 0.0267	Acc@1 95.059	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][200/397]	LR: 0.03000	Loss 0.0527	Acc@1 94.978	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][210/397]	LR: 0.03000	Loss 0.0994	Acc@1 95.053	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][220/397]	LR: 0.03000	Loss 0.0095	Acc@1 95.136	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][230/397]	LR: 0.03000	Loss 0.0282	Acc@1 95.184	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [2][240/397]	LR: 0.03000	Loss 0.0174	Acc@1 95.202	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [2][250/397]	LR: 0.03000	Loss 0.0062	Acc@1 95.244	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [2][260/397]	LR: 0.03000	Loss 0.0077	Acc@1 95.247	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [2][270/397]	LR: 0.03000	Loss 0.0454	Acc@1 95.261	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][280/397]	LR: 0.03000	Loss 0.0648	Acc@1 95.262	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [2][290/397]	LR: 0.03000	Loss 0.2778	Acc@1 95.264	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [2][300/397]	LR: 0.03000	Loss 0.0135	Acc@1 95.235	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][310/397]	LR: 0.03000	Loss 0.0391	Acc@1 95.217	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][320/397]	LR: 0.03000	Loss 0.0373	Acc@1 95.259	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [2][330/397]	LR: 0.03000	Loss 0.0092	Acc@1 95.289	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][340/397]	LR: 0.03000	Loss 0.0533	Acc@1 95.253	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [2][350/397]	LR: 0.03000	Loss 0.0579	Acc@1 95.228	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][360/397]	LR: 0.03000	Loss 0.0078	Acc@1 95.222	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][370/397]	LR: 0.03000	Loss 0.0116	Acc@1 95.241	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][380/397]	LR: 0.03000	Loss 0.0162	Acc@1 95.267	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [2][390/397]	LR: 0.03000	Loss 0.0036	Acc@1 95.261	Time 0.007 (0.007)	Data 0.000 (0.000)	
The sparsity of all parameters: name, num_nonzeros, total_num, shape, sparsity
fc1.weight: 49152, 49152, (512, 96), [0.0]
fc1.bias: 512, 512, (512,), [0.0]
fc2.weight: 3072, 3072, (6, 512), [0.0]
fc2.bias: 6, 6, (6,), [0.0]
---------------------------------------------------------------------------
0.0
===========================================================================


Training on 12685 examples
Task: 0, Epoch:2, Average Acc:[95.392], , Task Inc Acc:[98.070], Learning Acc:[95.392], Forgetting:[0.000], LR:0.03
	Acc@T0: 95.392	
	Til-Acc@T0: 98.070	

Training on 12685 examples
Epoch: [3][0/397]	LR: 0.03000	Loss 0.0254	Acc@1 96.875	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [3][10/397]	LR: 0.03000	Loss 0.0172	Acc@1 97.443	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [3][20/397]	LR: 0.03000	Loss 0.0085	Acc@1 96.577	Time 0.007 (0.008)	Data 0.000 (0.000)	
Epoch: [3][30/397]	LR: 0.03000	Loss 0.0573	Acc@1 95.867	Time 0.009 (0.008)	Data 0.000 (0.000)	
Epoch: [3][40/397]	LR: 0.03000	Loss 0.0116	Acc@1 95.732	Time 0.008 (0.008)	Data 0.000 (0.000)	
Epoch: [3][50/397]	LR: 0.03000	Loss 0.0403	Acc@1 95.588	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][60/397]	LR: 0.03000	Loss 0.0405	Acc@1 95.697	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][70/397]	LR: 0.03000	Loss 0.0424	Acc@1 95.731	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [3][80/397]	LR: 0.03000	Loss 0.0227	Acc@1 95.833	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][90/397]	LR: 0.03000	Loss 0.0218	Acc@1 95.776	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [3][100/397]	LR: 0.03000	Loss 0.0334	Acc@1 95.699	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][110/397]	LR: 0.03000	Loss 0.0061	Acc@1 95.749	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][120/397]	LR: 0.03000	Loss 0.0191	Acc@1 95.713	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [3][130/397]	LR: 0.03000	Loss 0.0135	Acc@1 95.635	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][140/397]	LR: 0.03000	Loss 0.0597	Acc@1 95.767	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [3][150/397]	LR: 0.03000	Loss 0.0435	Acc@1 95.778	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][160/397]	LR: 0.03000	Loss 0.0520	Acc@1 95.749	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][170/397]	LR: 0.03000	Loss 0.0066	Acc@1 95.742	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][180/397]	LR: 0.03000	Loss 0.0229	Acc@1 95.684	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][190/397]	LR: 0.03000	Loss 0.0487	Acc@1 95.599	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][200/397]	LR: 0.03000	Loss 0.0088	Acc@1 95.600	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][210/397]	LR: 0.03000	Loss 0.0370	Acc@1 95.586	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][220/397]	LR: 0.03000	Loss 0.0107	Acc@1 95.631	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][230/397]	LR: 0.03000	Loss 0.0207	Acc@1 95.671	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][240/397]	LR: 0.03000	Loss 0.0084	Acc@1 95.643	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][250/397]	LR: 0.03000	Loss 0.0472	Acc@1 95.642	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][260/397]	LR: 0.03000	Loss 0.0676	Acc@1 95.750	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][270/397]	LR: 0.03000	Loss 0.0744	Acc@1 95.710	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][280/397]	LR: 0.03000	Loss 0.0948	Acc@1 95.752	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][290/397]	LR: 0.03000	Loss 0.1164	Acc@1 95.769	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][300/397]	LR: 0.03000	Loss 0.0028	Acc@1 95.826	Time 0.009 (0.007)	Data 0.000 (0.000)	
Epoch: [3][310/397]	LR: 0.03000	Loss 0.0022	Acc@1 95.890	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [3][320/397]	LR: 0.03000	Loss 0.0131	Acc@1 95.911	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][330/397]	LR: 0.03000	Loss 0.0113	Acc@1 95.921	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][340/397]	LR: 0.03000	Loss 0.0099	Acc@1 95.986	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][350/397]	LR: 0.03000	Loss 0.0198	Acc@1 96.011	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [3][360/397]	LR: 0.03000	Loss 0.0040	Acc@1 96.018	Time 0.007 (0.007)	Data 0.000 (0.000)	
Epoch: [3][370/397]	LR: 0.03000	Loss 0.0561	Acc@1 96.066	Time 0.008 (0.007)	Data 0.000 (0.000)	
Epoch: [3][380/397]	LR: 0.03000	Loss 0.0079	Acc@1 96.112	Time 0.008 (0.007)	Data 0.000 (0.000)	
